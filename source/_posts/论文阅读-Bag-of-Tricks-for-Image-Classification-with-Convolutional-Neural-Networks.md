---
title: 论文阅读-Bag of Tricks for Image Classification with Convolutional Neural Networks
date: 2018-12-31 21:41:21
tags:
categories:
    - 论文
mathjax: true
---

## 主题

目前许多计算机视觉任务精度的提高得益于对训练过程的一些细微调整，这其中存在了许多小技巧，本文着重介绍了这些小技巧。
<!-- more -->
## 训练过程中的基本原则

1、 图像的预处理：

 1） 对图像随机取样并将数据转换为32位浮点型，像素值取值范围为0到255
 2） 随机裁剪长宽比在[3/4, 4/3]区域的矩形，随机取样的范围为[8%, 100%]，之后将图像放缩至224*224的大小
 3） 以0.5的概率水平翻转
 4） 缩放色调、饱和度、亮度，缩放系数在[0.6,1.4]中均匀采样
 5） 从正态分布中(0,0.01)中采样一个系数并加入pca噪声
 6）通过对RGB通道减去123.68、116.779、103.939，除以58.393/57.12,57.375来规范化图像数据

2、 对于验证集的处理，将图片短边缩放至256像素并保持其纵横比，之后以图像中心为中心裁剪至224*224并规范化，不使用其他变换。
3、 卷积层与全连接层权重初始化使用xavier算法，即权重在[-a,a]中随机取值，$a = \sqrt {6/({d_in} + {d_out})} $，这里d代表输入输出通道；所有的偏置项初始化为0，对于batch Normalization层，$\gamma$初始化为1，$\beta$初始化为0.
4、 训练中梯度下降方法使用NAG，训练模型以8块GPU训练120个epoch，batchsize为256，学习率初始为0.1，之后在第30个、60个、90个epoch时分别缩小10倍。

## 学习率调整策略

一般来说，我们倾向于使用较大的批量大小，这样可以提高训练效率，但大的批量大小却会导致收敛速度的减慢，为了解决这个矛盾，有以下几种方法：
 1） 学习率线性扩大
 大的批量意味着收敛速度的减慢，所以要增大学习率。在何恺明的实验中，批量大小为256的学习率为0.1，之后增大批量大小为b，则学习率变为0.1*b/256。
 2） 学习率预热
 开始训练时权重是随机的，这时如果使用较大的学习率会使得数值不稳定，所以采用的策略是开始学习率较低，训练几个batch后恢复初始学习率，即设置初始学习率为$\eta$，当第i个batch时，学习率为$i\eta /m$，其中$1 \le i \le m$。
 3）zero $\gamma $
 采用策略是开始训练时训练更少的层，即将残差块最后一个bn置于0。
 4）无偏衰减
 采用策略是只将L2正则化应用到权重上来避免过拟合，其他参数不经过正则化。

## 低精度训练

利用16位进行训练可以有效减少模型的训练时间

## 模型优化

如图所示：
![Resnet](/img/resnet.png)
![Resnet修改](/img/resnet1.png)
第一种修改是将3*3卷积核的步长修改为2,1*1卷积核的步长修改为1，这样可以避免输入内容的丢失。
第二种修改是将7*7卷积用3个3*3的卷积替换。
第三种修改是在1*1卷积前增加平均池化。

## 训练过程的微调

1、 余弦学习率下降，采用的公式为：
$${\eta_t} = \frac{1}{2}(1 + \cos (\frac{t\pi }{T}))\eta $$

学习率示意图如下：
![learn_rate](/img/learnrate.png)

2、 标签平滑

修改真实的概率，以减少分类层的过拟合可能性，是为了防止网络太过自信于自己的判断。注意，应用标签平滑应当是过拟合较为严重的时候，其他情况则不适用。

3、 知识蒸馏

使用一个效果较好的大的预训练模型来辅助训练一个小模型，损失函数为：
$$\ell (p,soft\max (z)) + {T^2}\ell (soft\max (r/T),soft\max (z/T))$$
注意，teacher模型跟student模型最好是同一种网络结构。

4、混合训练

即随机把两个样本加权线性插值，作为新样本用于训练：
$$\begin{array}{l}
\hat x = \lambda {x_i} + (1 - \lambda ){x_j},\\
\hat y = \lambda {y_i} + (1 - \lambda ){y_j}
\end{array}$$
