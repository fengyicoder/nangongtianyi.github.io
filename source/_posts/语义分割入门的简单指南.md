---
title: 语义分割入门的简单指南
date: 2019-04-15 18:18:26
tags:
categories:
    - 文章
---
语义分割是一种在图像上将标签分给每一个像素的过程。这与分类形成了鲜明的对比，分类任务是将单个的标签分配给整张图片。<!-- more -->语义分割将同一类的不同目标视为相同的对象。而实例分割则是将同一类的不同目标视为不同的对象（或者叫做实例）。通常情况下，实例分割比语义分割更加困难。本博客探讨了基于传统的与深度学习理论来进行语义分割的一些方法。而且，还讨论了一些较为流行的损失函数选择与应用。
![](/img/ss.png)

# 传统方法
在深度学习时代到来之前，我们使用了大量的图像处理技术将图像分割成许多感兴趣的的区域。我们将一些流行的方法罗列在下方：

## 灰度分割
最简单的语义分割方法是找到某个满足硬编码的规则或属性的区域，再为这个区域分配特定的标签。这种规则可以根据像素的属性来构建，例如它的灰度级。使用这种技术的一种方法是[Split and Merge](https://en.wikipedia.org/wiki/Split_and_merge_segmentation)算法。这种算法递归的将图像分割成子区域，直到分配了特定的标签，之后合并带有相同标签的相邻区域。

这种算法的问题在于其规则必须是一种硬编码。而且，仅仅使用灰度信息来表示诸如人类这样的复杂类别是极其困难的。因此，我们需要使用特征提取与优化技术来学习这样复杂类别的正确表示。

## 条件随机场
我们考虑通过训练一个模型给每个像素分类来分割图像。如果模型不够完美，我们可能会获得一个不那么自然的嘈杂分割结果（例如下图所示，狗跟猫的像素混在一起）。
![](/img/catmixdog.png)
这种事情可以通过考虑像素之间的先验关系来避免，比如目标的像素是连续的，因此相邻的像素趋向于有着相同的标签。为了建模这种关系，我们考虑使用条件随机场（CRFs）。

CRFs是一类针对于结构化预测的概率建模方法。与离散的分类器不同，CRFs在做出预测之前，会考虑近邻语义，例如像素之间的关系。这就使这种方法成了语义分割的理想选择。本小节探讨了CRFs在语义分割中的使用。

图像中的每一个像素都与一组可能状态的有限集有关。在我们的例子中，标签是可能状态的集合。将一种状态分配给一个像素所花费的代价称为unary cost。为了建模像素间的关系，我们将一对标签$(u,v)$分配给一对像素$(x,y)$的代价称为pairwise cost。我们可以从成对的像素来考虑，把它们看作直接的近邻（Grid CRF），或者我们可以从图像的所有像素来考虑（Dense CRF）。
![](/img/crf.jpeg)
所有像素的unary cost和pairwise cost之和作为CRF的energy(或者叫做代价或损失)。通过最小化上述值来获得较好的分割结果。

# 深度学习方法
深度学习极大的简化了语义分割的方法，并产生了极高质量的结果。在本章节中，我们讨论了训练这些深度学习方法的流行的模型架构与损失函数。

## 1. 模型结构
用于语义分割最流行最简单的架构是全卷积网络(FCN)。在论文[FCN for Semantic Segmentation](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf)中，作者使用了FCN通过卷积操作下采样了图像到一个较小的尺寸（同时得到了更多的通道），这些卷积的集合一般被称为编码器。之后，编码的结果要么经过双线性插值，要么经过一系列的转置卷积，来进行上采样操作。这些转置卷积的集合一般被称作解码器。
![](/img/fcn_.png)
尽管这种基础的架构非常有效，但也有一些缺点。一种缺点就是由于这些转置卷积（或者叫做反卷积）操作会输出不均匀的重叠，会存在棋盘伪像的现象。
![](/img/chess.png)
另一个缺点是在编码的过程中损失了信息，从而在边缘处分辨率较差。

之后有学者提出了一些改善基本FCN模型性能的方法，以下是一些被证明有效的较为流行的解决方案。
### U-Net
[U-Net](https://arxiv.org/abs/1505.04597)是简单的FCN架构的升级版本。在卷积块的输出和与之相匹配的转置卷积的输入之间，Unet网络加入了跳跃连接。
![](/img/Unet.png)
这种跳跃连接使梯度能够更好的流动，并提供了来自不同尺度图片的信息。较大尺度图片的信息（来自浅层）可以帮助模型分类更加精准，小尺度的信息（来自深层）可以帮助模型分割或者定位的更好。
### Tiramisu Model
[Tiramisu](https://arxiv.org/abs/1611.09326)模型与Unet较为相似，不同之处在于这种模型使用了[DenseNet](https://arxiv.org/pdf/1608.06993.pdf)中的Dense blocks来做卷积与转置卷积。一个Dense block有一些卷积层组成，前面所有层的特征图都作为后面层的输入。这种合成的网络具有极高的参数效率，可以更好的利用前面一些层的特征。
![](/img/t.png)
这种方法也存在缺点。由于ML框架之间的连接操作，这种方法的内存效率不是很高（需要较大的GPU才能运行）。
### 多尺度方法
一些深度学习模型明确的使用了一些方法来整合多尺度的信息。举例来说，[PSPNet](https://arxiv.org/pdf/1612.01105.pdf)使用四种不同的卷积核大小和步长来对卷积网络的输出做池化操作，之后使用双线性插值将所有池化的输出与卷积层的输出特征图进行上采样，在通道维度上连接它们。最后对这个连接的输出进行卷积产生最终的预测结果。
![](/img/PSP.png)
空洞卷积是一种结合多尺度信息的有效方法，而且其没有增加参数的数量。通过调整空洞率，相同滤波器的权重在空间上可以分布的更广，这使得网络可以学习到更加全局化的上下文语义信息。
![](/img/AC.png)
[DeepLabv3](https://arxiv.org/pdf/1706.05587.pdf)使用了不同空洞率的空洞卷积来捕获不同尺度的信息，而且没有明显的损失图像的分辨率。他们实验了一种级联的空洞卷积，也实验了一种并联的方式，被称为空洞空间金字塔池化（如下图所示）。
![](/img/aspp.png)
### 混合CNN-CRF方法
一些方法使用了CNN作为特征提取器，然而将输出的特征作为unary cost(potential)输入到了密集的CRF中。由于这种混合的CNN-CRF方法具有较强的像素关系建模能力，因此能得到较好的结果。
![](/img/crf.png)
而某些方法直接将CRF嵌入到了神经网络中，如同在[CRF-as-RNN](https://www.robots.ox.ac.uk/~szheng/papers/CRFasRNN.pdf)中密集CRF可以被RNN建模。如上图所示，这使得端到端训练成为可能。
## 2.损失函数
与普通的分类不同，语义分割必须挑选一些不同的损失函数。下面是一些语义分割较为常用的损失函数：
### Pixel-wise Softmax with Cross Entropy
语义分割标签的尺寸必须与原始图片相同。标签可以被编码成one-hot形式，如下图所示：
![](/img/onehot.jpeg)
由于标签是一种比较方便的one-hot形式，它可以直接被用来和ground truth（目标）一起计算交叉熵。然而，在求交叉熵之前，必须对预测得到的各个像素值执行softmax操作，这是因为每一个像素值都可以代表一类目标。
### Focal Loss
[RetinaNet](https://arxiv.org/pdf/1708.02002.pdf)提出的Focal Loss是标准交叉熵的升级版，可以用于一些极端的类不平衡的例子中。

考虑如下图所示的标准交叉熵（蓝色），即使我们的模型对某个像素值的类别非常自信（达到80%），它仍然具有较大的损失值（大约0.3）。另一方面，当模型对某一类置信度非常高时，Focal Loss（紫色，gamma值为2）不会对模型进行惩罚（当置信度为80%时，损失几乎为0）。
![](/img/focal.png)

让我们用一个直观的例子来探索为什么Focal Loss效果如此显著。假设我们有一张总共10000个像素点的图片，图片只有两类：背景（0）和目标（1）。我们假设图片的97%是背景，目标只有3%。现在，假定我们的模型确定了80%的背景，但只确定了30%的目标。

当我们使用交叉熵的时候，背景像素的损失为$97\%  \times 10000 \times 0.3 = 2850$，目标像素的损失值为$3\%  \times 10000 \times 1.2 = 360$。显然，置信度高的类（即背景类）的损失占主导地位，模型缺乏学习目标类的动力。作为对比，当使用focal loss时，背景类产生的损失值为$97\%  \times 10000 \times 0 = 0$，这使得模型能更好的学习目标类。

### Dice Loss
Dice Loss是另一种处理语义分割类不平衡的较为流行的损失函数。这种损失函数由[V-Net](https://arxiv.org/pdf/1606.04797.pdf)提出，可以用来计算预测与ground truth之间重叠的部分。Dice系数表示如下：
![](/img/dice1.png)
我们的目的是使预测与真值的重叠部分最大化。因此，我们通常最小化$(1-D)$来达到相同的目的，因为大多数的机器学习库只提供最小化的选项。
![](/img/dice2.png)
尽管Dice Loss对于类不平衡的样本效果较好，但我们可以看到其梯度计算公式的分母存在平方项。当某些值很小时，我们可能获得较大的梯度，从而导致训练不稳定。

# 应用
语义分割有各种各样的实际生活应用，下面是一些比较常见的用例。

## 自动驾驶
通常使用语义分割来识别车道、车辆、行人和其他感兴趣的目标。分割的结果用来对如何驾驶车辆做出正确的决策。
![](/img/autovehicles.png)

自动驾驶的一个限制是要求操作必须是实时的。上述问题的解决方案是将GPU与车辆本地集成到一起。为了增强上述方案的可操作性，可以使用更加轻量化的网络（较少的参数），或者可以使用在一种[极限](https://heartbeat.fritz.ai/how-to-fit-large-neural-networks-on-the-edge-eb621cdbb33)拟合神经网络的技术。

## 医疗图像分割
人们通常使用语义分割来识别医学扫描图中的显著性元素。主要用于识别类似于肿瘤的异常。算法的准确率和低召回率对于这种应用来说非常关键。
![](/img/m.png)
我们还可以自动执行一些不太重要的操作，例如从3D语义分割扫描图像中估计器官的体积。

## 景物理解
语义分割通常是复杂任务的基础，例如场景理解和视觉问答。场景图或者标题通常是场景理解算法的输出。
![](/img/su.png)
## Fashion Industry
服装行业通常使用语义分割从图像中提取衣服的像素块，用来给零售店提供一些相似的建议。更高级的算法可以给图像中的人物进行重新装扮。
![](/img/dress.png)
## 卫星图像处理
卫星图像使用语义分割来识别陆地类型。比较典型的例子包括分割水体来提供准确的地图信息。其他高级的用例包括绘制道路，识别作物类型，识别免费停车位等等。
![](/img/satel.png)

# 结论
深度学习极大的增强和简化了语义分割算法，为算法的实际应用铺平了道路。本博客列出的概念并非详尽无遗，因为研究团队正努力提高这些算法的准确性与实时性。无论未来发展如何，这篇博客介绍了当下这些算法一些流行的变式以及实际的应用。

本文译自[A Simple Guide to Semantic Segmentation](https://medium.com/beyondminds/a-simple-guide-to-semantic-segmentation-effcf83e7e54?sk=3d1a5a32a19d611fbd81028cfd4f23fd)
