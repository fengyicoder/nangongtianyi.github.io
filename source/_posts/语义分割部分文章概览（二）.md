---
title: 语义分割部分文章概览（二）
date: 2018-12-22 13:25:07
tags:
categories:
    - 论文
mathjax: true
---
此部分是关于部分语义分割论文的大致浏览
<!-- more -->

## 五、Dual Attention Network for Scene Segmentation
### 目的：

场景分割是一项挑战性的任务，因为其蕴含的语义信息十分丰富，很难提取出这些丰富的语义来实现完美的分割。文中提出了一种双重注意力网络来试图解决这个问题。

### 相关信息：

1. 提取丰富语义信息的一种方式是进行多尺度上的融合，这包括空洞金字塔池化，使用大的卷积核，或者利用编码译码结构来融合浅层与深层的语义信息。
2. 另一种方式是通过RNN，来获取长距离的依赖，从而获得更丰富的语义信息，但这种方式太过依赖于网络长期记忆的学习结果。
3. 注意力机制可以对长期依赖进行建模，后来出来的自注意力机制可以不引入外部信息，从自身出发，通过全局来增强内部的依赖性。

### 网络设计：
1. 网络结构如下：
![DANet](/img/DANet.png)
网络使用了基于空洞卷积的FCN,首先使用Resnet网络下采样八分之一大小，之后通过并联的两个注意力模块增强语义信息，之后通过卷积操作输出结果。
2. 空间注意力模块：
![PA](/img/PA.png)
首先将特征图复制三次，将其拉长为向量，通过B、C的相乘得到行列都为(H*W)的Gram矩阵，通过第三个复制向量D得到注意力图，之后进行加权增强语义的依赖。如下：
$${E_j} = \alpha \sum\limits_{i = 1}^N (s_{ji}{D_i}) + {A_j} $$
3. 通道注意力模块：
![CA](/img/CA.png)
基本思想与空间注意力模块大致相同，不做描述。

## 六、Pyramid Attention Network for Semantic Segmentation
### 目的：

本文的目的是为了更好的利用全局信息，从而对不同尺度的目标进行分割，作者设计了一种金字塔注意力网络来获取更加丰富的语义信息，最终得到更加精细的分割结果。

### 相关信息：

目前的分割网络普遍存在两个问题：
 1. 小尺度目标不容易检测，经常出现轮廓分割的较好，但分类出现错误的问题，deeplab对此的解决方案是使用空洞卷积，但会产生grid artifacts，而PSPNet则是使用了全局平均池化，但这样一来位置信息会丢失。
 2. 高级语义信息对低级语义信息没有帮助，一般来说高级语义信息有利于分类，而低级语义信息则保留了较多的位置信息。

### 网络设计

1. 网络为了规避deeplab的缺陷，放弃了空洞卷积，同时为了译码时生成较好的结果，利用高层语义信息来指导低层语义分辨率的恢复。网络总体设计如下：
![PANET](/img/PAN1.png)
2. 网络放弃了空洞卷积，但为了获取更加丰富的语义信息，这里利用了一种金字塔结构来进行多尺度信息的获取，同时增加了全局平均池化作为辅助。
![FPA](/img/PAN2.png)
3. 目前已知缓慢增加图片的分辨率恢复的效果较好，同时，文中设计了一种注意力机制，通过高级语义信息生成关于通道的注意力，依次来指导图片的恢复。
![GAU](/img/PAN3.png)

## 七、Adaptive Affinity Fields for Semantic Segmentation

### 目的：

语义分割能够实现像素级的分割，但对于一些语义信息不明显的区域区分不够好，如前景与背景太过相似，就难以实现较好的分割效果。论文为了解决这个问题，引入了对结构的推理，提出了自适应相似场。在实质上，其实是对损失函数的修改，将整个问题看作一个最优化问题。

### 相关信息：

1. 目前也有一些利用结构推理来帮助分割的方法，例如CRF，可以从视觉上来匹配目标的相似度，可以被用作分割网络的后处理阶段，用来改善分割效果，但花费的推理时间较长，同时对变化比较敏感；GAN也可以用来改善分割效果，但不易训练，同时容易出现模式崩溃。

### 解决方案

1. 目前大多数的分割网络的损失函数都是以像素点为对象的交叉熵损失函数，这样就只关注了像素，而缺失了对像素之间关系的考量，例如一辆车行驶在路上，我们很容易从两者的关系而确定目标是车辆。因此，文章在损失函数中引入了像素点的邻域。具体公式见论文。

2. 具体来说，其使用的相似域损失函数是一种基于KL散度的相似域损失函数，详情参见论文。这种损失函数并非定义在图像上，而是定义在groundtruth上，通过groundtruth的引导，强迫同一类的聚合在一起，不同类的分离，而不管图像是什么样的。值得一提的是这个过程只发生在训练阶段，因此不耗费推理的时间。

3. 对于CNN来说，一般都要求同样大小的卷积核，这其实是不合理的，论文提出了一种自适应的相似域损失函数，给与了不同类别不同的权重。具体公式见论文。

4. 这样直接进行优化，会导致得到的最优解是一个非常小的值，不能达到自适应的目的，因此文中考虑了一种极端的方式，即在最坏的情况下找到最好的值，将其表述为一个极小极大问题，具体公式见论文。
PS:这篇论文的公式太难打了，博客都显示不了，最后只能全部删除。
