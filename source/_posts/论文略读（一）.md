---
title: 论文略读（一）
date: 2021-01-22 20:24:36
tags:
categories:
    - 论文
mathjax: true
---

# DiCENet: Dimension-wise convolutions for efficient networks

## 主题

文章提出了一种分维度卷积之后维度特征融合的一种新模块，相对于经典的深度可分离模块（典型网络为MobileNet、ShuffleNet）来说性能更好，消耗资源更少。

<!-- more -->

## 相关信息

CNN的核心是卷积层，但卷积的进行会消耗大量的计算资源，因此出现了许多量化压缩的方法。flattened convolution通过对长宽通道三个维度依次卷积来近似标准卷积，但忽略了各维度之间的关系，因此无法在各种计算机视觉任务中推广；深度可分离卷积将卷积分解成立深度和点卷积，提高了效率，被诸多典型的网络所应用。

此外，神经架构搜索也是近来新兴的寻找网络最优架构的方法。网络的量化、压缩与蒸馏也可以用来提高网络的效率，与文章所述方法存在正交性。

## 内容

网络架构如图所示：

![DiCE unit](/img/dice.png)

网络采用三条分支来分别对三个维度进行编码，深度采用卷积核为${k_D} \in \mathbb{R}^{1 \times n \times n}$，宽度采用卷积核为${k_W} \in {\mathbb{R}^{n \times n \times 1} }$，高度采用卷积核为${k_H} \in {\mathbb{R}^{n \times 1 \times n} }$，产生的特征图为：
$$
{Y_{Dim} } = \{ {Y_D},{Y_W},{Y_H}\}  \in {\mathbb {R}^{3D \times H \times W} }
$$
之后对三个维度的信息进行融合，这里分为两个步骤，分别是局部融合与全局融合。这里的局部融合将特征图看作$D$个组的张量，每组有三个维度的信息，因此使用了${k_g} \in {\mathbb{R}^{3 \times 1 \times 1} }$的卷积核，产生特征图${Y_G} \in {\mathbb{R}^{D \times H \times W} }$。由于是卷积核对$D$组特征图分别进行处理，因此这里看作是局部融合操作。全局融合则是分别学习特征图空间与深度两个维度的语义表示，之后将其融合，具体来说就是在空间上使用了${k_S} \in {\mathbb{R}^{1 \times n \times n} }$大小的卷积核学习空间语义；深度上收到了SE unit的启发，使用了两个全连接层来学习深度的语义信息，为了学习到非线性的语义关系，中间还使用了ReLU激活函数，最后，使用深度语义信息来给空间语义信息加权，完成了全局信息的融合。

# Refining activation downsampling with SoftPool Alexandros

## 主题

CNN使用池化来减小特征图的大小，池化操作能够在局部实现空间不变性，还能够增大感受野。本文提出了一种新型的快速有效的池化方法，与其他方法相比，该方法能够在下采用的过程中保留更多的信息。

## 相关信息

下采样已经被广泛应用在手动提取特征的方法中，例如bag-of-words和bag-of-features，在这些方法中，图像被视为局部块的集合，将其池化或编码成向量。

CNN中的池化方法主要有以下几种：

![CNN_pooling](E:\project\nangongtianyi.github.io\source\img\CNN_pooling.png)

- Average Pooling：区域平均值
- Max Pooling：区域最大值
- Stochastic Pooling：使用一个核区域内激活的概率加权抽样
- Mix Pooling：最大池化和平均池化的混合
- Power average Pooling：最大池化与平均池化的结合，采用学习参数$p$来控制两者的比重，当$p=1$时等于局部求和，当$p = \infty $，等于最大池化
- S3 Pooling：对原始feature map网格中的行和列进行随机采样
- Preserving Pooling：使用平均池化，同时使用高于平均值的值增强激活
- Local Importance Pooling：

# Puzzle-CAM: Improved localization via matching partial and full features

## 主题

弱监督的语义分割主要用来缩小图像级语义分割与像素级语义分割的差距，然而