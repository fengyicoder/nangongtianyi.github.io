<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Programmer-and-Mathematics-二]]></title>
    <url>%2F2021%2F01%2F30%2F%C2%96Programmer-and-Mathematics-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[论步伐与耐心在上一章中，我们学到了很多东西，最为突出的是我们了解了学习陌生的数学知识时会有多慢。作者提到，每次看到定义或定理时，都必须停住写下来以便好好的理解。这与编程没什么不同，有经验的开发者都知道何时启动REPL或调试器，或者写测试程序来隔离新功能，测试其工作原理。 对我们来说，数学与编程最大的区别就是数学没有REPL或者调试器，没有参考实现。数学家经常通过相互交流来绕过这个障碍，作者鼓励我们找一个朋友一起通读这本书（:)似不似为了多卖两本书）。William Thurston在《关于数学的证明和进步》中写道：“数学知识嵌入到了思考某个主题的人们的思想和社会结构中，书籍和论文对此提供了支持，但是您走的越高，知识的主要来源与教科书的距离就越远。” 如果你是自己阅读这本书，那么你必须扮演如下的角色：开发者、测试人员和编译器。当你想出新的点子提出问题时，你是一个开发者，当你阅读定理和定义时，你是一个测试人员，当你检查你的直觉和预感是否有错误时，你是一个编译器。对新手和专家而言，这通常会使你阅读数学的速度变慢。数学家们通常使用铅笔和笔记本来非常方便的阅读。 当你初读定理时，你会感到非常困惑。作者再次提到：铁律就是你会困惑，例外是你什么都懂。数学文化的特点就是你什么都不懂时会感到十分的舒适。但这是一种卑微的生活。一旦你弄明白了一些不清楚的地方，你的理解就会取得进步。最容易的方式就是写下大量的示例，但并不是总能做到这一点。我们已经看到了这样的例子，那就是不可能有一个非零多项式其根的个数大于其次数。 作者在这里举了一个例子，这里不再引述，意在阐述这样一个事实：当你尝试学习的数学越多，你越会觉得自己越一无所知。我们也要尝试做到这一点：我显然没搞明白，但我已经接受了，会慢慢尝试理解。 作者在这里又举了几个例子，当我们想学习数学，而这些知识又十分庞大，可能需要花费我们大量的时间时，请务必记住两个事情：洞察力像一个阶梯，每一个梯级都是有用的，所以不要怕一些小事特别花时间，只要它是有用的就行；当我们练习阅读和吸收的数学知识越多，我们掌握的就越好，因为在这个过程中，会锻炼我们解决问题的能力。 最为重要的一点：兴趣是最好的老师。 集合本章将为本书的其余部分奠定基础，这章的大部分专门讨论集合和集合之间函数的数学语言。集合和函数不仅仅是计算机科学中大多数数学的基础，还是所有数学家之间的通用语言。集合是数学的建模语言，将现实世界的问题转换成数学语言的第一种，也通常是最简单的方式就是以集合和函数的方式写下问题的核心概念。然而集合论中有许多新的术语，理解这些的最好方式就是写下来大量的示例。 将想法转换为集合的语言后，就可以利用许多现有的工具和技术来处理集合。这样一来，所要做的工作就是从数学的角度来了解这些技术。软件也大致是这样，我们需要学习如何将复杂的问题分解成为简单的、可测试的、可维护的功能，无论使用哪一种编程语言都可。软件随业务的变化而灵活对业务规则进行建模的过程也是如此，集合是一项基本的能力。 在本章的最后，我们将看到名为稳定婚姻的app的完整建模过程，该过程是数学和经济学跨学科领域的一部分。在经济学中，有时市场不能将金钱用作交换媒介，在这些情况下，必须寻找其他机制使市场有效运作。我们将看到的示例是医疗居住匹配市场，但类似的想法也适用于器官捐赠和住房分配等市场。正如我们所看到的那样，对这些系统进行建模以使其能够进行数学分析，只需要我们能够流利的使用集合和函数。 集合，函数以及它们的关系集合是唯一对象的集合。我们应该已经在软件中见过类似的概念，例如python中的set，Java中的HashSet以及C++中的unordered_set，功能上它们是等效的，都是没有重复的对象集合。在本章中，不讨论集合的技术细节，即不讨论对象的进入离开，只关心数学集合的相关特性。 开始了解集合的第一件事就是如何描述集合，大多数方法都是隐式的，而其中最简单的就是用语言描述，例如能被7整除的整数集等。分析数学对象的目的经常是找到一种比隐式描述更好的使用集合来进行更有效描述的方法，但从隐式定义开始研究集合是一个不错的起点。 描述集合的一种较为熟悉的方法是使用集合生成器符号。这种风格对开发人员比较友好，因为这种形式存在于多种编程语言中，例如定义所有可能被7整除的非负数的集合： S = \{ x:x \in \mathbb{N},{\text{x is divisible by 7} }\}定义4.1：集合$A$的基数或大小，用$|A|$表示，是集合$A$中元素有限时元素的个数，否则$A$有无效的基数。基数为0的集合称为空集。 定义4.2：集合$B$中的元素也是集合$A$的元素，则集合$B$是$A$的子集，表示为$B \subset A$；如果两个集合元素相同，则两个元素相等，即$B \subset A$且$A \subset B$，则$A=B$。 定义4.3：给定两个集合$A$和$B$，$B$在$A$中的补集为${ a \in A:a \notin B} $，也可以表示为$A\backslash B$或者$A-B$，当$B \subset A$也可以表示成${B^C}$。 定义4.4：给定集合$A,B$，并集为$A \cup B$，代表${x:x \in A{\text{ or } }x \in B}$，交集为$A \cap B$代表${x:x \in A{\text{ and } }x \in B} $。 定义4.5：集合$A,B$的积用$A \times B$表示，代表： A \times B = \{(a,b):a \in A{\text{ and }}b \in B\}积是一种将实线$\mathbb{R}$转换为$\mathbb{R}^2$很好用的方式，定义${\mathbb{R}^2} = \mathbb{R} \times \mathbb{R}$，同理${\mathbb{R}^3} = \mathbb{R} \times \mathbb{R} \times \mathbb{R}$，这里可能会有点有疑惑，那就是$\mathbb{R}^3$如何展开，第一种可能性是： (\mathbb{R} \times \mathbb{R}) \times \mathbb{R} = \{ ((a,b),c):a \in \mathbb{R},b \in \mathbb{R},c \in \mathbb{R}\}第二种可能性是： \mathbb{R} \times (\mathbb{R} \times \mathbb{R}) = \{ (a,(b,c)):a \in \mathbb{R},b \in \mathbb{R},c \in \mathbb{R}\}作为开发人员，很明显这两者是不同的，编译器会将上述两者认为是两种不同的东西，但作为数学家，出于某种原因，忽略了差异，认为两者是相同的，即： (\mathbb{R} \times \mathbb{R}) \times \mathbb{R} = \mathbb{R} \times (\mathbb{R} \times \mathbb{R}) = \{ (a,b,c):a \in \mathbb{R},b \in \mathbb{R},c \in \mathbb{R}\}定义4.6：$A$和$B$是集合，$F$是$A \times B$的子集，如果$F$满足以下特性我们称它是一个函数：对任意$a \in A$都有唯一的$(a,b) \in F$；集合$A$称为$F$的域，集合$B$称为$F$的共域，用符号表示为：$f:A \to B$。 到目前为止，我们已经看到了许多定义的示例。实际上，我们将函数在计算上视为输入到输出的映射。下面举一个简单的例子： F = \{ (1,1),(2,4),(3,9),(4,16), \ldots \} = \{ (x,{x^2}):x \in \mathbb{N}\}这个集合是$\mathbb{N} \times \mathbb{N}$的子集，现在我们可以对$(3,9) \in F$写下新的符号，使用映射的符号表示$F(3)=9$，这样，我们就可以按照我们想要的方式来描述$F$，即$F(x)=x^2$。通过定义4.6可以确保每个输入$x$都有某个输出$F(x)$，且只有一个输出$F(x)$。提供具体的算法来计算输出使得上述的条件微不足道，同时也不需要算法来定义函数。 至于为什么使用集合的术语来定义函数，部分是由于历史的原因，这里不对此做陈述，只需要知道集合是数学的基础即可。对于我们来讲，我们将函数看作与常规集合不同，函数具有语义上的输入输出依赖关系，我们将集合视作一种工具，用于帮助我们阐明概念或者消除一些定义中的歧义。现在，我们来看一下一些有关函数输入输出子集的有关定义。 定义4.7：给定函数$f:A \to B$，我们定义$f$的像为： f(A)=\{f(a):a \in A\}定义4.8：$A,B$是集合，$f:A \to B$是函数，有$b \in B$，$b$在$f$下的原像为${f^{ - 1}}(b)$，以集合形式表达为${a \in A: f(a)=b}$。类似的，如果$C \subset B$是子集，则${f^{ - 1}}(C)$定义为${ a \in A: f(a) \in C}$。 定义4.9：如果$a,a’ \in A$不同，且$B$中的元素$f(a),f(a’)$也不相同，则函数$f:A \to B$称为单射。 单射的示意图如下，是一种一一对应的关系： 定义4.10：如果对于任一$b \in B$，总有至少一个$a \in A$使得$f(a)=b$，这样的函数$f:A \to B$叫做满射；换句话说如果$f$的像为$B$，那么$f$是满射。 满射的示意图如下： 如果$f$既是单射又是满射，那么$f$就是双射，也叫做一一映射。仅双射存在逆。仅给出函数的描述来计算逆是非常困难的。实际上，大多数密码学都是基于这样的假设：某些函数在计算上不可逆；但另一方面，线性代数中计算矩阵的逆是可行的，因此，研究逆的概念也是有必要的。 命题4.11：逆是唯一的。 证明：假定$f:A \to B$是双射且${g_1}:B \to A$与${g_2}:B \to A$都是逆，假定$\forall b \in B$，有$f(a) = b$，则${g_1}(b) = {g_1}(f(a)) = a$，同理有${g_2}(b) = {g_2}(f(a)) = a$，则$g_1$与$g_2$相同。 命题4.12：假定$A,B$是集合，$f:A \to B$是双射，若$\forall a \in A$有$g(f(a)) = a$，$\forall b \in B$有$f(g(b)) = b$，则$g:B \to A$是$f$的逆。 现在稍微解释以下上文我们为什么视为$(\mathbb{R} \times \mathbb{R}) \times \mathbb{R} = \mathbb{R} \times (\mathbb{R} \times \mathbb{R})$，最根本的原因是存在双射$(\mathbb{R} \times \mathbb{R}) \times \mathbb{R} \to \mathbb{R} \times (\mathbb{R} \times \mathbb{R})$可以将$((a,b),c)$映射为$(a,(b,c))$。当数学家们想要称呼两个事物相同时，他们会想出这样的双射，认为双射两边应该被认为是相同的。 聪明的双射和计数现在我们已经了解了关于集合的一些基本的描述语言，可以对一些问题进行建模了。假定现在想要计算集合的大小，由于集合可以隐式定义，可能不是那么容易计算。一种好用的方法是找到一种巧妙的双射，可以将看似困难的计数问题转变为优雅有技巧的问题。 来看第一个示例，假设你正在举办网球比赛，比赛是单淘汰赛，意味着两名选手结束比赛时，胜者留下，败者则退出比赛。作为比赛的主持人，你需要了解各种事情，比如完成比赛需要多长时间，要同时进行多少场比赛等等。要解决这些问题我们需要一个最基本的数据：一共有多少场比赛？也就是说给定由这个复杂的过程生成的比赛的集合，我们想要计算其大小。 假定我们有一千名选手，现在来探讨一下如何计算这一点。在第一轮，每个选手都会与其他选手配对，一共有500场比赛。第二轮中，剩下的500名选手再次配对参加比赛，一共有250场，在第三轮，有125场。在第四轮则会遇到问题，现在选手是奇数个了，那必然会出现选手轮空。好，现在比赛继续进行，最终会得到一个总数。这个答案是999，比选手的人数少1，这是巧合吗？对于别的比赛人数也是如此吗？ 答案是肯定的，为了证明这一点，这里展示了找到巧妙双射的技巧。我们只需要观察到这样一个事实，每个选手都输掉一场比赛，所以如果我们要计算比赛次数，只需要计算失败选手的人数，只有一个没有失败的选手：胜利者，所以有999场比赛。 让我们把这个问题用集合的语言来描述，如果$X$是比赛的集合，而$Y$是选手的集合，我们可以定义这样一个函数$f:X \to Y$，称$f(x)$是关于失败者$x$的函数。$f(X)$的像是失败者的子集$L \subset Y$，实际上，$f$定义了关于$X$和$L$的双射，这意味着$X$和$L$有相同的大小。而$|L| = |Y| - 1$，所以有$n$个选手则有$n-1$场比赛。 为了确保理解这个过程，将示例扩展成双淘汰赛，即选手输了两场才会被淘汰。那么现在对任意的$y \in Y$有${f^{ - 1} }(y) = { x \in X:f(x) = y} $的大小为2，胜利者有可能一场没输或者只输了一场，这种情况下肯定不是双射，那么就无法计算出确切的场数，但可以得到一个大概的边界。 这种常规的策略可以在你需要计数或者估计集合大小的时候使用。想象一下，你想估计城市中无家可归的人数，则必须找到一种巧妙的方法，通过观察他们的行为的残存的影响来隐式的对其计数，这恰恰是在要计算的集合接近双射或者双倍或三倍覆盖时寻找函数。 以下是找到巧妙双射的另一个例子。给定集合$X$，我们假定数量： 代表从$X$中选择2个，有如下的定义： 如果$X$是有限集，其大小为$n$，那么从这$n$个对象中挑选2个，很显然一共有$\frac{ {n(n - 1)} }{2}$种方式。接下来，我们将尝试用双射的方式来计算。图示如下： 我们设置$n=7$，淡黄色的球为$Y$，最后一行$X$有$n$个方块。图示向我们展示了如何定义一个双射$g:Y \to C_X^2$，很显然，我们能从图中得到答案$1 + 2 + \cdots + (n - 1)$。 归纳和矛盾证明接下来我们会接触两种严格的证明方法，一种是归纳法，也就是递归。我们是这样理解递归的：函数能够以更小的参数集来调用自身，并以基本情况来处理允许的最小参数，经典的例子是斐波那契数列： fib(n) = fib(n - 1) + fib(n - 2)同时$fib(0) = fib(1) = 1$。 与其相似，归纳法是一种证明方法，当要证明一个命题时，允许使用最基本的情况、更简单的参数来使用这个命题最终达到证明这个命题的目的。困难在于何时何地使用归纳法，通常是在证明所有自然数适用的命题时，例如：对于所有整数$n \geqslant 6$都有$P(n)$为true。归纳证明分两步进行： 首先列出最基本的情况，对于此例来说是$P(6)$为true。 之后是归纳步骤，使用$P(n)$为true来证明$P(n+1)$为true，同理，也可以使用$P(n-1)$来证明$P(n)$。 像递归一样，我们可以得到一连串的证明：$P(6)$隐含着$P(7)$为true，一直到$P(n)$。 现在我们利用归纳法来证明： 先从最基本的情况开始，由于$n \geqslant 2$，当$n$为2时，答案为1，成立。另外，如果： 则必有： 假设集合$X = { 1,2, \ldots ,n + 1} $，现在将其分成两部分，一部分是两个元素都从$Y = X - { n + 1} = { 1,2, \ldots ,n} $中选择。另一部分则是取的两个元素有一个是$n+1$，显然，两部分加起来就是我们要证明的问题。 第一部分，有： 答案应为命题假设所述$1 + 2 + \cdots + n - 1$。 第二部分更为简单，我们取的方法只有$n$种。合在一起，一共有$1 + 2 + \cdots + n$种，命题得证。有趣的是，归纳证明在数学上的声誉很差，因为其往往不能向读者传达任何见识，我们对上述的证明也不如使用图像示意来得直观。 第二种证明的方法是矛盾证明，有如下的例子：你正在参加派对，出于好奇，你问你的朋友他在聚会上有几个朋友，他数了数一共有五个。同时，你也数了数发现自己也有五个朋友。真是巧合，现在你处于数学家的敏锐洞察力觉得这可能有深层的原因，于是你调查了一番，发现其他人在聚会上也有五个朋友。那么，每个聚会都是这样吗？聚会上是否总会有至少两个人有相同数量的朋友？答案是肯定的。 证明如下：假设有聚会每个人都有不同数目的朋友，假设聚会总共有$n$个人，每个人的朋友数目在0和$n-1$之间，这是一个闭区间。由于有$n$个人，而且有$n$个不相等的数字，其属于$[0,n-1]$。我们可以将每个人映射到其所拥有的朋友数量，并且该映射是双射。现在就出现了一个矛盾，有人必须有0个朋友，而有人必须有$n-1$个朋友，这意味着他必须是所有人的朋友，这显然矛盾，因此假设不成立。 使用矛盾进行证明的目的是使对象具有某种可以使用的属性，如果你试图证明不存在具有某些特殊属性的对象，你可以通过矛盾证明假设这种对象存在，使用特殊属性继续进行证明。 应用：稳定的婚姻问题如下：现在有$n$位男性和女性，最终的目标是选择谁应该嫁给谁，如果我们将$M$称为男性，$W$称为女性，我们的目的是找到双射$M \to W$来描述婚姻。 给出以下稳定的定义： 定义4.13：双射$f:M \to W$稳定，意味着没有成对的$m \in M$与$w \in W$满足以下两个条件： $f(m) \ne w$，意味着两者不匹配。 比起他们分配的对象，成对者更喜欢对方，即$ran{k_m}(w) &lt; ran{k_m}(f(m))$和$ran{k_w}(m) &lt; ran{k_w}({f^{ - 1}}(w))$。 现在，算法问题是给定男女双方的偏好列表作为输入，我们是否可以找到稳定的婚姻？我们可以保证对任何一个偏好表稳定的婚姻都存在吗？答案都是肯定的，这使用了延迟接受的算法。 这是算法的非正式描述：轮流进行，在每轮中，每个男性都向尚未拒绝他的自己最喜欢的女性求婚，另一方面，女性如果在本轮中被求婚，她会拒绝每个求婚人，不过她最喜欢的除外，但她也不会接受，会推迟到最后接受。最拒绝的男性很伤心，但他们在下一轮会继续向下一个喜欢的女性求婚。到了最后，没有人会剩下，这些女性的选择会构成一个稳定的双射。 用程序表示如下，suitors向suiteds求婚： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class Suitor: def __init__(self, id, preference_list): """ A Suitor consists of an integer id (between 0 and the total number of Suitors), and a preference list implicitly defining a ranking of the set of Suiteds. E.g., Suitor(2, [5, 0, 3, 4, 1, 2]) says the third Suitor prefers the Suited with index 5 the most, then the Suited with index 0, etc. The Suitor will propose in decreasing order of preference, and maintains the internal state index_to_propose_to to keep track of the next proposal. """ self.preference_list = preference_list self.index_to_propose_to = 0 self.id = id def preference(self): return self.preference_list[self.index_to_propose_to] def post_rejection(self): self.index_to_propose_to += 1 def __eq__(self, other): return isinstance(other, Suitor) and self.id == other.id def __hash__(self): return hash(self.id) def __repr__(self): return "Suitor(&#123;&#125;)".format(self.id) class Suited: def __init__(self, id, preference_list): self.preference_list = preference_list self.held = None self.current_suitors = set() self.id = id def reject(self): """Return the subset of Suitors in self.current_suitors to reject, leaving only the held Suitor in self.current_suitors. """ if len(self.current_suitors) == 0: return set() self.held = min(self.current_suitors, key=lambda suitor: self.preference_list.index(suitor.id)) rejected = self.current_suitors - set([self.held]) self.current_suitors = set([self.held]) return rejected def add_suitor(self, suitor): self.current_suitors.add(suitor) def __eq__(self, other): return isinstance(other, Suited) and self.id == other.id def __hash__(self): return hash(self.id) def __repr__(self): return "Suited(&#123;&#125;)".format(self.id) 以下是延迟接受算法的主要代码： 12345678910111213141516171819202122232425def stable_marriage(suitors, suiteds): """ Construct a stable marriage between Suitors and Suiteds. Arguments: suitors: a list of Suitor suiteds: a list of Suited, which deferred acceptance of Suitors. Returns: A dict &#123;Suitor: Suited&#125; matching Suitors to Suiteds. """ unassigned = set(suitors) while len(unassigned) &gt; 0: for suitor in unassigned: next_to_propose_to = suiteds[suitor.preference()] next_to_propose_to.add_suitor(suitor) unassigned = set() for suited in suiteds: unassigned |= suited.reject() for suitor in unassigned: suitor.post_rejection() # have some ice cream return dict([(suited.held, suited) for suited in suiteds]) 定理4.14：延迟接受算法总是能够终止，并且最终能够产生稳定的双射。 证明这里不再赘述，可以参见对应章节。]]></content>
      <categories>
        <category>文章</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[论文略读（一）]]></title>
    <url>%2F2021%2F01%2F22%2F%E8%AE%BA%E6%96%87%E7%95%A5%E8%AF%BB%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[DiCENet: Dimension-wise convolutions for efficient networks主题文章提出了一种分维度卷积之后维度特征融合的一种新模块，相对于经典的深度可分离模块（典型网络为MobileNet、ShuffleNet）来说性能更好，消耗资源更少。 相关信息CNN的核心是卷积层，但卷积的进行会消耗大量的计算资源，因此出现了许多量化压缩的方法。flattened convolution通过对长宽通道三个维度依次卷积来近似标准卷积，但忽略了各维度之间的关系，因此无法在各种计算机视觉任务中推广；深度可分离卷积将卷积分解成立深度和点卷积，提高了效率，被诸多典型的网络所应用。 此外，神经架构搜索也是近来新兴的寻找网络最优架构的方法。网络的量化、压缩与蒸馏也可以用来提高网络的效率，与文章所述方法存在正交性。 网络架构网络架构如图所示： 网络采用三条分支来分别对三个维度进行编码，深度采用卷积核为${k_D} \in \mathbb{R}^{1 \times n \times n}$，宽度采用卷积核为${k_W} \in {\mathbb{R}^{n \times n \times 1} }$，高度采用卷积核为${k_H} \in {\mathbb{R}^{n \times 1 \times n} }$，产生的特征图为： {Y_{Dim} } = \{ {Y_D},{Y_W},{Y_H}\} \in {\mathbb {R}^{3D \times H \times W} }之后对三个维度的信息进行融合，这里分为两个步骤，分别是局部融合与全局融合。这里的局部融合将特征图看作$D$个组的张量，每组有三个维度的信息，因此使用了${k_g} \in {\mathbb{R}^{3 \times 1 \times 1} }$的卷积核，产生特征图${Y_G} \in {\mathbb{R}^{D \times H \times W} }$。由于是卷积核对$D$组特征图分别进行处理，因此这里看作是局部融合操作。全局融合则是分别学习特征图空间与深度两个维度的语义表示，之后将其融合，具体来说就是在空间上使用了${k_S} \in {\mathbb{R}^{1 \times n \times n} }$大小的卷积核学习空间语义；深度上收到了SE unit的启发，使用了两个全连接层来学习深度的语义信息，为了学习到非线性的语义关系，中间还使用了ReLU激活函数，最后，使用深度语义信息来给空间语义信息加权，完成了全局信息的融合。 Refining activation downsampling with SoftPool Alexandros主题CNN使用池化来减小特征图的大小，池化操作能够在局部实现空间不变性，还能够增大感受野。本文提出了一种新型的快速有效的池化方法，与其他方法相比，该方法能够在下采用的过程中保留更多的信息。 相关信息下采样已经被广泛应用在手动提取特征的方法中，例如bag-of-words和bag-of-features，在这些方法中，图像被视为局部块的集合，将其池化或编码成向量。 CNN中的池化方法主要有以下几种： Average Pooling：区域平均值 Max Pooling：区域最大值 Stochastic Pooling：使用一个核区域内激活的概率加权抽样 Mix Pooling：最大池化和平均池化的混合 Power average Pooling：最大池化与平均池化的结合，采用学习参数$p$来控制两者的比重，当$p=1$时等于局部求和，当$p = \infty $，等于最大池化 S3 Pooling：对原始feature map网格中的行和列进行随机采样 Preserving Pooling：使用平均池化，同时使用高于平均值的值增强激活 Local Importance Pooling：]]></content>
      <categories>
        <category>论文</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Programmer and Mathematics (一)]]></title>
    <url>%2F2021%2F01%2F09%2FProgrammer-and-Mathematics-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[多项式从加密开始，假设现在有一条秘密消息，将其分成10个部分进行编码，接收方只要知道任意6个部分，就可以重建消息，但如果少于6个部分，甚至不能确定原始消息的一小段，要实现这种方案，多项式足以。 定理2.1：实系数单变量多项式是输入为实数输出也为实数的一种函数，形式如下： f(x) = {a_0} + {a_1}x + {a_2}{x^2} + ... + {a_n}{x^n}其中${a_i}$是$f(x)$的系数，且必须是非负整数，多项式的次数为$n$。 从程序语言来讲，一个关于系数的数组就可以限定一个多项式，例如a[i]就代表${a_i}$，数组的长度就代表多项式的次数。 多项式是非常重要的，其内部的乘加特性，可以延申到所有的算数运算。现在暂时扩展到多变量，在程序语言中的与或非，就是非常简单的多变量多项式，但最终能产生全部的算法范围。 考虑一种特殊情况，虽然次数存在，但系数全部为0。在这种情况下，虽然是合理的但次数就失去了其应有的意义，因此，“按惯例”，最后一个系数${a_n}$应该是非零的，同时，特别定义$f(x) = 0$是零多项式，其次数为-1。 当定义一个函数的时候通常使用如下的表示：$f:A \to B$。所有可能的输入被称作域，所有可能的输出被称作范围。在数学中，范围是所有真实输出的集合，输出的类型称为共域。在上述的表达中，我们指定$A$代表域，$B$代表共域。 了解了一些基本的概念之后，可以着手实现前文所述的加密，这里需要利用到以下的定理：经过给定点集的多项式的存在性与唯一性定理。 定理2.2：对任意整数$n \ge 0$和属于$\mathbb{R}^{2}$的$n+1$个点的点集$({x_0},{y_0}),({x_1},{y_1}),…,({x_n},{y_n})$，其中${x_0} &lt; {x_1} &lt; {x_2} &lt; … &lt; {x_n}$，存在一个唯一的$n$次多项式，使得对所有的$i$有$p({x_i}) = {y_i}$。 其中，$\mathbb{R}^{2}$代表了一对实数，每个实数都属于$\mathbb{R}$，类似的，$\mathbb{Z}^{3}$代表一个整数的三元组。 这个定理可以用一种更简短的方式陈述：存在唯一的$n$次多项式，其经过了$n+1$个点的选择。当看见一个新定理的时候，要做的第一件事就是写下一个最简单的例子，这样除了能够简化定理，还能够提供示例，为审阅证明定理的时候使用。例如对以上的定理选择$(7,4)$这个例子，这是一个零次多项式，函数唯一且确定，为$f(x)=4$。接下来稍微复杂一点，考虑数据为$(2,3),(7,4)$，这是一个一次多项式，也能很容易的确定这个多项式应为$f(x) = \frac{ {13} } {5} + \frac{1} {5}x$。在几何上，次数为1的多项式是一条直线。这是显而易见的，在两点之间有唯一的一条直线。此外，定理还指出了${x_0} &lt; {x_1} &lt; {x_2} &lt; … &lt; {x_n}$，那么来考虑下${x_0} = {x_1}$的情况，例如$(2,3),(2,5)$。 我们不考虑真实答案，只猜测可能没有次数为1的多项式经过这两点，或者有多个次数为1的多项式经过这两点，那么就破坏了唯一性。因此，我们应该尝试以一种更加精确的方式阅读定义。 现在考虑证明2.2，将包括两个部分，存在性和唯一性。首先，我们将证明存在一个满足要求的多项式，然后证明如果两个多项式都满足要求，则它们必须相同。下面将通过直接构造的方式来证明存在性。与前文类似，我们还是从最简单的方式开始，但是是以一种通用的形式。此时如果是一个点$(x_1, y_1)$，则多项式为$f(x) = {y_1}$，两个点的情况我们直接写成以下的形式$f(x) = {y_1}\frac{ {x - {x_2} } } { { {x_1} - {x_2} } } + {y_2}\frac{ {x - {x_1} } } { { {x_2} - {x_1} } }$，显然，满足要求，且式中已经包含了${x_1} \ne {x_2}$。将其简化为多项式的标准形式： f(x) = \frac{ { {x_1} {y_2} - {x_2} {y_1} } } { { {x_1} - {x_2} } } + (\frac{ { {y_1} - {y_2} } } { { {x_1} - {x_2} } })x显然，$x$的幂没有出现大于1的情况，而且没有出现两个$x$相乘的情况，那么，可以确信此多项式的次数为1. 三个点的多项式，我们也可以按照以上的方式进行构造： f(x) = {y_1}\frac{ {(x - {x_2})(x - {x_3})} } { {({x_1} - {x_2})({x_1} - {x_3})} } + {y_2}\frac{ {(x - {x_1})(x - {x_3})} } { {({x_2} - {x_1})({x_2} - {x_3})} } + {y_3}\frac{ {(x - {x_1})(x - {x_2})} } { {({x_3} - {x_1})({x_3} - {x_2})} }同理，$n+1$个点的多项式就很容易构造了，如下： f(x) = \sum\limits_{i = 0}^n { {y_i}(\prod\limits_{j \ne i} {\frac{ {x - {x_j} } } { { {x_i} - {x_j} } } } )}此时，其实已然证明了存在性，因为每一项都是$n$次的，这显然是一个$n$次的多项式。 下面展示以下上式中某些数学符号的代码展示，有以下的例子： f(x) = \sum\limits_{i = 0}^n {bar(i)(\prod\limits_{j \ne i} {foo(i,j)} )}123456789101112int i, j;sometype theSum = defaultSumValue;for (i = 0; i &lt;= n; i++) &#123; othertype product = defaultProductValue; for (j = 0; j &lt;= n; j++) &#123; if (j != i) &#123; product *= foo(i, j); &#125; &#125; theSum += bar(i) * product;&#125;return theSum; 下面证明唯一性，依赖如下定理： 定理2.3：零多项式是实数内唯一一个次数最多为$n$，但有超过$n$个不同根的多项式。 现在来进行证明，假设有两个$n$次的多项式，经过点集$({x_0},{y_0}),({x_1},{y_1}),…,({x_n},{y_n})$，这两个多项式可以相同也可以不同，我们所要做的是证明这两个多项式必须相同，那么就证明了唯一性。现在构造这样一个多项式$(f - g)(x)$，定义$(f - g)(x) = f(x) - g(x)$，如果$f$的系数为$a_i$，$g$的系数为$b_i$，那么$f-g$的系数为$c_i=a_i-b_i$，显然$f-g$是一个多项式。我们对这个多项式了解多少呢，首先，这个多项式的次数最多是$n$，而且我们知道$(f - g)({x_i}) = 0$。现在我们使用定义2.3，我们假定$f-g$的次数为$d \leqslant n$，我们知道此多项式的根不超过$n$个，除非它是零多项式，但有$n+1$个点（即上文所述点集中的点）可以使上式成立，因此这是一个零多项式，因此$f$跟$g$相同。 既然已经通过给定点集证明了$n$次多项式的存在性和唯一性，那么就可以为其命名，成为给定点的插值多项式，插值意味着获取点集，并找到通过这些点的唯一最小次多项式。 代码实现以下将编写一个插值点的python程序，将假设存在一个多项式类，该类接受一系列参数并生成多项式，如下： 1234567891011def interpolate(points): &quot;&quot;&quot;Return the unique polynomial of degree at most n passing through the given n+1 points.&quot;&quot;&quot; if len(points) == 0: raise ValueError(&apos;Must provide at least one point.&apos;) x_values = [p[0] for p in points] if len(set(x_values)) &lt; len(x_values): raise ValueError(&apos;Not all x values are distinct.&apos;) terms = [single_term(points, i) for i in range(0, len(points))] return sum(terms, ZERO) 输入点集后，通过single_term函数生成单项，将这些单项相加即是多项式，生成单项的代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243def single_term(points, i): &quot;&quot;&quot;Return one term of an interpolated polynomial. This method computes one term of the sum from the proof of Theorem 2.2. In particular, it computes: y_i \product_&#123;j=0&#125;^n (x - x_i) / (x_i - x_j) The encapsulating `interpolate` function sums these terms to construct the final interpolated polynomial. Arguments: - points: a list of (float, float) - i: an integer indexing a specific point Returns: A Polynomial instance containing the desired product. &quot;&quot;&quot; the_term = Polynomial([1.]) xi, yi = points[i] for j, p in enumerate(points): if j == i: continue xj = p[0] &quot;&quot;&quot; The inlined Polynomial instance below is how we represent (x - x_i) / (x_i - x_j) using our Polynomial data type (where t is the variable, and x_i, x_j are two x-values of data points): (x - x_i) / (x_i - x_j) = (-x_j / (x_i - x_j)) * t^0 + (1 / (x_i - x_j)) * t^1 &quot;&quot;&quot; the_term = the_term * Polynomial([-xj / (xi - xj), 1.0 / (xi - xj)]) # Polynomial([yi]) is a constant polynomial, i.e., we&apos;re scaling the_term # by a constant. return the_term * Polynomial([yi]) 这里利用了上文所述的构造方法，每一项为${y_i}(x - {x_j})/({x_i} - {x_j})$，这里将其分解成了${a_0} = - {x_j}/({x_i} - {x_j})$和${a_1} = 1/({x_i} - {x_j})$来生成一个简单的多项式。 实践以下将使用多项式插值来实现加密。假设武林盟主有五个得力手下，他向他们分享一个秘密，秘密以二进制表示，也许这个秘密是一张藏宝图。问题在于这些手下是贪婪的，如果直接把秘密给他们，或许有人会直接独吞藏宝图；如果只给一部分，他们也有可能直接找到藏宝图，更糟糕的是，如果三个人聚在一起，很可能会猜中剩余的部分，排除掉另外两人。因此，为了安全，这个秘密应该具有以下的属性： 每个人只有获得了部分秘密，且这部分是独一无二的； 如果任意四个人聚在一起共享秘密，他们也不能得到完整的秘密； 如果五个人全部聚在一起，他们才能得到完整的秘密，拿到藏宝图。 实际上，任意四个人聚在一起不只是不能获取完整的秘密，他们甚至破译不出任何秘密。神奇的是是有这种方案的，无论有多少个人（$n$），或者是希望有多少组（$k$）能重建秘密，这都是可能的。以下给出这种方案，即使用多项式插值。我们将秘密用整数$s$表示，有$f(0)=s$，现在我们想让五个手下齐聚能得到秘密，因此$k$就是5，那么我们要构造的多项式的次数就是$k-1$也就是4，系数随机生成即可，然后将$f(1),f(2),f(3),f(4),f(5)$分发作为秘密的部分分发给手下，这样，只有五个人齐聚才能拿到藏宝图。]]></content>
      <categories>
        <category>文章</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[语义分割部分文章概览（四）]]></title>
    <url>%2F2019%2F05%2F23%2F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E9%83%A8%E5%88%86%E6%96%87%E7%AB%A0%E6%A6%82%E8%A7%88%EF%BC%88%E5%9B%9B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[此部分是关于部分语义分割论文的大致浏览 十一、FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation目的：在网络的设计中，空洞卷积虽然能够增大网络的感受野，但一般在使用空洞卷积的时候不进行下采样操作，这就导致了计算资源跟内存的占用。本文作者力求在使用下采样的情况下，通过设计一个联合上采样金字塔模块，逼近使用空洞卷积的效果，这样效果没有变差，而且计算量跟存储占用大大减小。 设计思想：经过分析，作者发现空洞卷积操作相当于将先将输入特征图进行拆分，再进行卷积操作，之后合并即可，而跨步卷积则是相当于先进行卷积操作，之后进行reduce操作。如上图所示，这里利用的是一种联合上采样的思想，不管是空洞卷积还是跨步卷积，其输入的特征图是一致的。对跨步卷积来说，当进行了reduce操作之后，相当于执行了一个下采样操作，之后经过不断的卷积得到最终的结果。而空洞卷积则是不进行下采样，直接进行卷积操作，相当于直接对高分辨率的输入特征图进行处理。我们知道，对于联合上采样来说，可以通过低分辨的输入输出得到一个映射，而这个映射能近似的将高分辨率的输入映射到高分辨率的输出。本文正是利用了这样一种思想来模拟一些分割网络中的空洞卷积的设计。 网络设计：首先将特征图转换到同一个嵌入空间，方便进行融合处理，之后通过扩张率不同的空洞卷积学习低分辨的输入与低分辨率输出的映射，即学习$y_m^0$与$y_s$之间的映射关系，而并行的普通卷积则学习$y_m^0$与$y_m$之间的关系，这样一个过程模拟一个映射，这个映射由低分辨率得到，用来处理高分辨率的输入，为了完成对空洞卷积的模拟，还需要对得到的特征图进行合并，这里进行了通道的连接，最后利用一个卷积将学到的特征转换成最终的输出。]]></content>
      <categories>
        <category>论文</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[语义分割入门的简单指南]]></title>
    <url>%2F2019%2F04%2F15%2F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E5%85%A5%E9%97%A8%E7%9A%84%E7%AE%80%E5%8D%95%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[语义分割是一种在图像上将标签分给每一个像素的过程。这与分类形成了鲜明的对比，分类任务是将单个的标签分配给整张图片。语义分割将同一类的不同目标视为相同的对象。而实例分割则是将同一类的不同目标视为不同的对象（或者叫做实例）。通常情况下，实例分割比语义分割更加困难。本博客探讨了基于传统的与深度学习理论来进行语义分割的一些方法。而且，还讨论了一些较为流行的损失函数选择与应用。 传统方法在深度学习时代到来之前，我们使用了大量的图像处理技术将图像分割成许多感兴趣的的区域。我们将一些流行的方法罗列在下方： 灰度分割最简单的语义分割方法是找到某个满足硬编码的规则或属性的区域，再为这个区域分配特定的标签。这种规则可以根据像素的属性来构建，例如它的灰度级。使用这种技术的一种方法是Split and Merge算法。这种算法递归的将图像分割成子区域，直到分配了特定的标签，之后合并带有相同标签的相邻区域。 这种算法的问题在于其规则必须是一种硬编码。而且，仅仅使用灰度信息来表示诸如人类这样的复杂类别是极其困难的。因此，我们需要使用特征提取与优化技术来学习这样复杂类别的正确表示。 条件随机场我们考虑通过训练一个模型给每个像素分类来分割图像。如果模型不够完美，我们可能会获得一个不那么自然的嘈杂分割结果（例如下图所示，狗跟猫的像素混在一起）。这种事情可以通过考虑像素之间的先验关系来避免，比如目标的像素是连续的，因此相邻的像素趋向于有着相同的标签。为了建模这种关系，我们考虑使用条件随机场（CRFs）。 CRFs是一类针对于结构化预测的概率建模方法。与离散的分类器不同，CRFs在做出预测之前，会考虑近邻语义，例如像素之间的关系。这就使这种方法成了语义分割的理想选择。本小节探讨了CRFs在语义分割中的使用。 图像中的每一个像素都与一组可能状态的有限集有关。在我们的例子中，标签是可能状态的集合。将一种状态分配给一个像素所花费的代价称为unary cost。为了建模像素间的关系，我们将一对标签$(u,v)$分配给一对像素$(x,y)$的代价称为pairwise cost。我们可以从成对的像素来考虑，把它们看作直接的近邻（Grid CRF），或者我们可以从图像的所有像素来考虑（Dense CRF）。所有像素的unary cost和pairwise cost之和作为CRF的energy(或者叫做代价或损失)。通过最小化上述值来获得较好的分割结果。 深度学习方法深度学习极大的简化了语义分割的方法，并产生了极高质量的结果。在本章节中，我们讨论了训练这些深度学习方法的流行的模型架构与损失函数。 1. 模型结构用于语义分割最流行最简单的架构是全卷积网络(FCN)。在论文FCN for Semantic Segmentation中，作者使用了FCN通过卷积操作下采样了图像到一个较小的尺寸（同时得到了更多的通道），这些卷积的集合一般被称为编码器。之后，编码的结果要么经过双线性插值，要么经过一系列的转置卷积，来进行上采样操作。这些转置卷积的集合一般被称作解码器。尽管这种基础的架构非常有效，但也有一些缺点。一种缺点就是由于这些转置卷积（或者叫做反卷积）操作会输出不均匀的重叠，会存在棋盘伪像的现象。另一个缺点是在编码的过程中损失了信息，从而在边缘处分辨率较差。 之后有学者提出了一些改善基本FCN模型性能的方法，以下是一些被证明有效的较为流行的解决方案。 U-NetU-Net是简单的FCN架构的升级版本。在卷积块的输出和与之相匹配的转置卷积的输入之间，Unet网络加入了跳跃连接。这种跳跃连接使梯度能够更好的流动，并提供了来自不同尺度图片的信息。较大尺度图片的信息（来自浅层）可以帮助模型分类更加精准，小尺度的信息（来自深层）可以帮助模型分割或者定位的更好。 Tiramisu ModelTiramisu模型与Unet较为相似，不同之处在于这种模型使用了DenseNet中的Dense blocks来做卷积与转置卷积。一个Dense block有一些卷积层组成，前面所有层的特征图都作为后面层的输入。这种合成的网络具有极高的参数效率，可以更好的利用前面一些层的特征。这种方法也存在缺点。由于ML框架之间的连接操作，这种方法的内存效率不是很高（需要较大的GPU才能运行）。 多尺度方法一些深度学习模型明确的使用了一些方法来整合多尺度的信息。举例来说，PSPNet使用四种不同的卷积核大小和步长来对卷积网络的输出做池化操作，之后使用双线性插值将所有池化的输出与卷积层的输出特征图进行上采样，在通道维度上连接它们。最后对这个连接的输出进行卷积产生最终的预测结果。空洞卷积是一种结合多尺度信息的有效方法，而且其没有增加参数的数量。通过调整空洞率，相同滤波器的权重在空间上可以分布的更广，这使得网络可以学习到更加全局化的上下文语义信息。DeepLabv3使用了不同空洞率的空洞卷积来捕获不同尺度的信息，而且没有明显的损失图像的分辨率。他们实验了一种级联的空洞卷积，也实验了一种并联的方式，被称为空洞空间金字塔池化（如下图所示）。 混合CNN-CRF方法一些方法使用了CNN作为特征提取器，然而将输出的特征作为unary cost(potential)输入到了密集的CRF中。由于这种混合的CNN-CRF方法具有较强的像素关系建模能力，因此能得到较好的结果。而某些方法直接将CRF嵌入到了神经网络中，如同在CRF-as-RNN中密集CRF可以被RNN建模。如上图所示，这使得端到端训练成为可能。 2.损失函数与普通的分类不同，语义分割必须挑选一些不同的损失函数。下面是一些语义分割较为常用的损失函数： Pixel-wise Softmax with Cross Entropy语义分割标签的尺寸必须与原始图片相同。标签可以被编码成one-hot形式，如下图所示：由于标签是一种比较方便的one-hot形式，它可以直接被用来和ground truth（目标）一起计算交叉熵。然而，在求交叉熵之前，必须对预测得到的各个像素值执行softmax操作，这是因为每一个像素值都可以代表一类目标。 Focal LossRetinaNet提出的Focal Loss是标准交叉熵的升级版，可以用于一些极端的类不平衡的例子中。 考虑如下图所示的标准交叉熵（蓝色），即使我们的模型对某个像素值的类别非常自信（达到80%），它仍然具有较大的损失值（大约0.3）。另一方面，当模型对某一类置信度非常高时，Focal Loss（紫色，gamma值为2）不会对模型进行惩罚（当置信度为80%时，损失几乎为0）。 让我们用一个直观的例子来探索为什么Focal Loss效果如此显著。假设我们有一张总共10000个像素点的图片，图片只有两类：背景（0）和目标（1）。我们假设图片的97%是背景，目标只有3%。现在，假定我们的模型确定了80%的背景，但只确定了30%的目标。 当我们使用交叉熵的时候，背景像素的损失为$97\% \times 10000 \times 0.3 = 2850$，目标像素的损失值为$3\% \times 10000 \times 1.2 = 360$。显然，置信度高的类（即背景类）的损失占主导地位，模型缺乏学习目标类的动力。作为对比，当使用focal loss时，背景类产生的损失值为$97\% \times 10000 \times 0 = 0$，这使得模型能更好的学习目标类。 Dice LossDice Loss是另一种处理语义分割类不平衡的较为流行的损失函数。这种损失函数由V-Net提出，可以用来计算预测与ground truth之间重叠的部分。Dice系数表示如下：我们的目的是使预测与真值的重叠部分最大化。因此，我们通常最小化$(1-D)$来达到相同的目的，因为大多数的机器学习库只提供最小化的选项。尽管Dice Loss对于类不平衡的样本效果较好，但我们可以看到其梯度计算公式的分母存在平方项。当某些值很小时，我们可能获得较大的梯度，从而导致训练不稳定。 应用语义分割有各种各样的实际生活应用，下面是一些比较常见的用例。 自动驾驶通常使用语义分割来识别车道、车辆、行人和其他感兴趣的目标。分割的结果用来对如何驾驶车辆做出正确的决策。 自动驾驶的一个限制是要求操作必须是实时的。上述问题的解决方案是将GPU与车辆本地集成到一起。为了增强上述方案的可操作性，可以使用更加轻量化的网络（较少的参数），或者可以使用在一种极限拟合神经网络的技术。 医疗图像分割人们通常使用语义分割来识别医学扫描图中的显著性元素。主要用于识别类似于肿瘤的异常。算法的准确率和低召回率对于这种应用来说非常关键。我们还可以自动执行一些不太重要的操作，例如从3D语义分割扫描图像中估计器官的体积。 景物理解语义分割通常是复杂任务的基础，例如场景理解和视觉问答。场景图或者标题通常是场景理解算法的输出。 Fashion Industry服装行业通常使用语义分割从图像中提取衣服的像素块，用来给零售店提供一些相似的建议。更高级的算法可以给图像中的人物进行重新装扮。 卫星图像处理卫星图像使用语义分割来识别陆地类型。比较典型的例子包括分割水体来提供准确的地图信息。其他高级的用例包括绘制道路，识别作物类型，识别免费停车位等等。 结论深度学习极大的增强和简化了语义分割算法，为算法的实际应用铺平了道路。本博客列出的概念并非详尽无遗，因为研究团队正努力提高这些算法的准确性与实时性。无论未来发展如何，这篇博客介绍了当下这些算法一些流行的变式以及实际的应用。 本文译自A Simple Guide to Semantic Segmentation]]></content>
      <categories>
        <category>文章</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[论文阅读-Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation]]></title>
    <url>%2F2019%2F03%2F24%2F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Auto-DeepLab-Hierarchical-Neural-Architecture-Search-for-Semantic-Image-Segmentation%2F</url>
    <content type="text"><![CDATA[研究目标神经架构搜索的网络已经在图像任务中取得了比人类设计的网络更好的成绩，由此，作者提出将神经架构搜索应用到语义分割领域，来获取较优的分割结果。 亮点：1、 提出了网络级架构搜索空间来联合单元级架构搜索空间进行搜索，寻找更优的结构。 2、开发出了一个可微的连续公式，可以在两级分层架构上进行有效的搜索。 相关信息：1、神经架构搜索旨在自动设计网络结构，从而减少人工的时间与工作量。早期的工作大都试图直接建立整个网络，后来转向搜索可重复单元（例如resnet块），外部网络级结构则通过人类设计，即控制分层计算的内部单元级使用自动搜索，而控制分辨率的网络级则使用手动设计。 2、神经架构搜索在图像分类任务上取得了较大的成功，但直接应用在语义分割任务上则效果不佳，原因是NAS一般使用从低分辨率到高分辨率的迁移学习，而语义分割是密集型的预测任务，需要直接在高分辨率图像上进行学习，这就需要设计一个更加松弛的搜索空间来捕捉高分辨率的变化，同时需要更加高效的搜索技术，因为需要更多的计算。 网络设计1、单元级搜索空间：将一个小的全卷积模块定义为一个单元，一个单元是由B个块组成的有向无环图。每个块是双分支结构，单元$l$中的块$i$通常用一个5元组表示$({I_1},{I_2},{O_1},{O_2},C)$,其中$I_1$与$I_2$是输入张量的选择，$O_1$与$O_2$是输出张量的选择，$C$是组合两个分支的方法。单元的输出张量$H^l$是所有块输出张量${ H_1^l, \ldots ,H_B^l} $的组合。可能的输入张量是由前两个单元的输出以及当前单元当前块之前的输出组成。可能的层类型O为： 1）$3 \times 3$深度分离卷积 2）$5 \times 5$深度分离卷积 3）$3 \times 3$空洞卷积，空洞率为2 4）$5 \times 5$空洞卷积，空洞率为2 5）$3 \times 3$的平均池化 6）$3 \times 3$的最大池化 7）跳跃连接 8）无连接对于可能的组合操作$C$的集合，采用简单的对应元素相加的方法。由于以上给出的都是离散的方式，搜索空间较大，为了方便用梯度下降法求解，需要转换成连续空间的搜索。这里所作的操作是对两个分支分别加权: H_i^l = {\alpha ^1}*{O_1}({I_1}) + {\alpha ^2}*{O_2}({I_2})其中，${\alpha ^1} + {\alpha ^2} = 1$ 2、网络级搜素空间：所谓的网络级搜索空间，其实就是特征图分辨率改变步骤的选择，在由于密集图像预测的各种网络中，一般有如下的原则：1）下一层的空间分辨率要么是两倍大，要么是两倍小，要么保持不变2）最小的空间分辨率是原图像分辨率的下采样32倍3）开始第一层的特征图一般是原来的四倍小网络级搜索空间示例如下：为了建立连续松弛，每层$l$至多有四个隐藏状态$\ {}^4{H^l},{}^8{H^l},{}^{16}{H^l},{}^{32}{H^l}\ $，左上标表示空间分辨率，分辨率可能从三个地方进行转移，分别是分辨率是其一半的地方，分辨率相同的地方以及分辨率是其两倍的地方，分别给于三个$\beta $权重，且其和为1，所以我们可以将其视为转移概率，而要找到一条最大概率的路径可以通过维特比算法达成。]]></content>
      <categories>
        <category>论文</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[论文阅读-Deformable Convolutional Networks]]></title>
    <url>%2F2019%2F03%2F15%2F%C2%96%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%C2%96Deformable-Convolutional-Networks%2F</url>
    <content type="text"><![CDATA[目的：卷积核固定的尺寸在一定程度上限制了模型对于图像几何建模的能力，论文提出了可变形卷积来致力于解决这个问题。 相关信息：对于增强模型的几何建模能力，目前有两种比较普遍的方法：一是对数据进行相应的增强，即对图像进行各种几何变换来丰富数据集，再利用网络学习相应的几何建模能力；二是利用transformation-invariant特征与算法。但这两种方法都有相应的缺点，第一，我们内在假设了变换的先验已知，如果遇到了没有见过的几何变换，就无法进行几何建模，第二，手动设计的特征与算法无法处理过于复杂的变换。 网络设计：1、作者设计了一个可变形卷积模块，实质是卷积核不变，但采样点的位置发生偏移，变成了一种不规则的采样，以此来学习图像的几何变换，示意如下：卷积总共分两路，下面一路是标准的卷积，上面一路生成一定的偏移，之后与下路相加进行线性插值，确定采样点的坐标，再进行卷积。2、作者还设计了一种可变形RoI池化来代替普通的RoI池化，与1相似，也是偏移了采样点，示意如下：3、PS RoI池化设计如下：]]></content>
      <categories>
        <category>论文</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[语义分割部分文章概览（三）]]></title>
    <url>%2F2019%2F02%2F24%2F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E9%83%A8%E5%88%86%E6%96%87%E7%AB%A0%E6%A6%82%E8%A7%88%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[此部分是关于部分语义分割论文的大致浏览 八、Attention U-Net: Learning Where to Look for the Pancreas目的：简化用于医学成像的分割模型，加入注意力机制后可以取消额外的定位模块，且不耗费太大的运算量。 网络设计：为了突出用于分割的显著特征并且抑制不相关的特征，网络集成了注意力门控机制，且利用了unet网络，通过渐进的下采样获得不同尺度的特征，利用跳跃连接，使用了门控连接了粗细两个尺度的信息来提取更加丰富的语义信息用于分割，网络示意图如下： 九、LadderNet: Multi-path networks based on U-Net for medical image segmentation目的：改进U-Net网络，提高网络分割的精确度 网络设计：1、作者认为U-Net网络分割精度受限的原因是网络信息流动的路径有限，所以，为了提高网络的精度，作者增加了信息流动路径，提出了一种阶梯U-net网络，即多个U-Net网络的结合，网络结构如下：作者认为，这样的U-Net结构可以认为是许多个FCN网络的集合，当然能够提取更加丰富的特征，进而提高分割精度。2、此外，作者使用了共享参数的技巧，网络中的标准残差块参数共享，大大减少了网络本应有的参数与训练难度 十、CCNet: Criss-Cross Attention for Semantic Segmentation目的：使用长距离注意力依赖生成注意力图，利用注意力图辅助分割 网络设计：1、如果利用每个像素点与全局的关系来生成注意力，耗费的计算资源太大，所以作者在这里只利用了像素点所在位置的那一行与那一列，如下图所示：2、如果只利用交叉行的信息会使得语义信息太过稀疏，所以论文采用了一种循环的方式（这里暂时没看懂，后期补充）。]]></content>
      <categories>
        <category>论文</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[论文阅读-Bag of Tricks for Image Classification with Convolutional Neural Networks]]></title>
    <url>%2F2018%2F12%2F31%2F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Bag-of-Tricks-for-Image-Classification-with-Convolutional-Neural-Networks%2F</url>
    <content type="text"><![CDATA[主题目前许多计算机视觉任务精度的提高得益于对训练过程的一些细微调整，这其中存在了许多小技巧，本文着重介绍了这些小技巧。 训练过程中的基本原则1、 图像的预处理： 1） 对图像随机取样并将数据转换为32位浮点型，像素值取值范围为0到255 2） 随机裁剪长宽比在[3/4, 4/3]区域的矩形，随机取样的范围为[8%, 100%]，之后将图像放缩至224*224的大小 3） 以0.5的概率水平翻转 4） 缩放色调、饱和度、亮度，缩放系数在[0.6,1.4]中均匀采样 5） 从正态分布中(0,0.01)中采样一个系数并加入pca噪声 6）通过对RGB通道减去123.68、116.779、103.939，除以58.393/57.12,57.375来规范化图像数据 2、 对于验证集的处理，将图片短边缩放至256像素并保持其纵横比，之后以图像中心为中心裁剪至224*224并规范化，不使用其他变换。3、 卷积层与全连接层权重初始化使用xavier算法，即权重在[-a,a]中随机取值，$a = \sqrt {6/({d_in} + {d_out})} $，这里d代表输入输出通道；所有的偏置项初始化为0，对于batch Normalization层，$\gamma$初始化为1，$\beta$初始化为0.4、 训练中梯度下降方法使用NAG，训练模型以8块GPU训练120个epoch，batchsize为256，学习率初始为0.1，之后在第30个、60个、90个epoch时分别缩小10倍。 学习率调整策略一般来说，我们倾向于使用较大的批量大小，这样可以提高训练效率，但大的批量大小却会导致收敛速度的减慢，为了解决这个矛盾，有以下几种方法： 1） 学习率线性扩大 大的批量意味着收敛速度的减慢，所以要增大学习率。在何恺明的实验中，批量大小为256的学习率为0.1，之后增大批量大小为b，则学习率变为0.1*b/256。 2） 学习率预热 开始训练时权重是随机的，这时如果使用较大的学习率会使得数值不稳定，所以采用的策略是开始学习率较低，训练几个batch后恢复初始学习率，即设置初始学习率为$\eta$，当第i个batch时，学习率为$i\eta /m$，其中$1 \le i \le m$。 3）zero $\gamma $ 采用策略是开始训练时训练更少的层，即将残差块最后一个bn置于0。 4）无偏衰减 采用策略是只将L2正则化应用到权重上来避免过拟合，其他参数不经过正则化。 低精度训练利用16位进行训练可以有效减少模型的训练时间 模型优化如图所示：第一种修改是将33卷积核的步长修改为2,11卷积核的步长修改为1，这样可以避免输入内容的丢失。第二种修改是将77卷积用3个33的卷积替换。第三种修改是在1*1卷积前增加平均池化。 训练过程的微调1、 余弦学习率下降，采用的公式为： {\eta_t} = \frac{1}{2}(1 + \cos (\frac{t\pi }{T}))\eta学习率示意图如下： 2、 标签平滑 修改真实的概率，以减少分类层的过拟合可能性，是为了防止网络太过自信于自己的判断。注意，应用标签平滑应当是过拟合较为严重的时候，其他情况则不适用。 3、 知识蒸馏 使用一个效果较好的大的预训练模型来辅助训练一个小模型，损失函数为： \ell (p,soft\max (z)) + {T^2}\ell (soft\max (r/T),soft\max (z/T))注意，teacher模型跟student模型最好是同一种网络结构。 4、混合训练 即随机把两个样本加权线性插值，作为新样本用于训练： \begin{array}{l} \hat x = \lambda {x_i} + (1 - \lambda ){x_j},\\ \hat y = \lambda {y_i} + (1 - \lambda ){y_j} \end{array}]]></content>
      <categories>
        <category>论文</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[论文阅读-Group Normalization]]></title>
    <url>%2F2018%2F12%2F29%2F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Group-Normalization%2F</url>
    <content type="text"><![CDATA[问题的提出Batch Normalization虽然是一种较好的防止过拟合的方法，但也有其缺陷：那就是随着批次的减小，BN的错误率会增加，但在一些cv任务中，又要求较小的批次，因此需要解决这个问题，伦文提出了一种Group Normalization的方法。 相关工作 Batch Normalization问题的产生：归根结底是不同阶段数据分布的不一致造成的，通常来说，训练阶段跟测试阶段的batch size是不同的，而现在大多数网络在测试阶段都是直接调用了训练阶段产生的均值与方差，这显然就是不合理的，因为数据分布出现了变化。 LN与IN避开了batch维度方面，有一定的效果，但仍然比不上本文的GN。 BR一定程度上减弱了批次小错误率高的问题，但仍没有从根本上解决。 Group Normalization的设计 论文表明，神经网络中的特征图进行语义表示其实也是成组的，那么，就可以对这些成组的特征图一起规范化。 一般来说，规范化用公式表达如下：{\hat x_i} = \frac{1}{\sigma_i}({x_i} - {\mu_i})主要是计算均值与方差，规范化之后，为了补偿表示能力可能有的损失，进行了一个线性变换：{y_i} = \gamma {({\hat x}_i)} + \beta不同的方法进行规范化的方式如图所示： 总结通过实验的对比，GN有着不错的效果，但由于目前BN是训练网络的一种十分有效的方法，许多超参数与结构对BN来说可能是最好的，但对于GN来说却可能不是最合适的，需要重新设计结构。]]></content>
      <categories>
        <category>论文</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[语义分割部分文章概览（二）]]></title>
    <url>%2F2018%2F12%2F22%2F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E9%83%A8%E5%88%86%E6%96%87%E7%AB%A0%E6%A6%82%E8%A7%88%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[此部分是关于部分语义分割论文的大致浏览 五、Dual Attention Network for Scene Segmentation目的：场景分割是一项挑战性的任务，因为其蕴含的语义信息十分丰富，很难提取出这些丰富的语义来实现完美的分割。文中提出了一种双重注意力网络来试图解决这个问题。 相关信息： 提取丰富语义信息的一种方式是进行多尺度上的融合，这包括空洞金字塔池化，使用大的卷积核，或者利用编码译码结构来融合浅层与深层的语义信息。 另一种方式是通过RNN，来获取长距离的依赖，从而获得更丰富的语义信息，但这种方式太过依赖于网络长期记忆的学习结果。 注意力机制可以对长期依赖进行建模，后来出来的自注意力机制可以不引入外部信息，从自身出发，通过全局来增强内部的依赖性。 网络设计： 网络结构如下：网络使用了基于空洞卷积的FCN,首先使用Resnet网络下采样八分之一大小，之后通过并联的两个注意力模块增强语义信息，之后通过卷积操作输出结果。 空间注意力模块：首先将特征图复制三次，将其拉长为向量，通过B、C的相乘得到行列都为(H*W)的Gram矩阵，通过第三个复制向量D得到注意力图，之后进行加权增强语义的依赖。如下：{E_j} = \alpha \sum\limits_{i = 1}^N (s_{ji}{D_i}) + {A_j} 通道注意力模块：基本思想与空间注意力模块大致相同，不做描述。 六、Pyramid Attention Network for Semantic Segmentation目的：本文的目的是为了更好的利用全局信息，从而对不同尺度的目标进行分割，作者设计了一种金字塔注意力网络来获取更加丰富的语义信息，最终得到更加精细的分割结果。 相关信息：目前的分割网络普遍存在两个问题： 小尺度目标不容易检测，经常出现轮廓分割的较好，但分类出现错误的问题，deeplab对此的解决方案是使用空洞卷积，但会产生grid artifacts，而PSPNet则是使用了全局平均池化，但这样一来位置信息会丢失。 高级语义信息对低级语义信息没有帮助，一般来说高级语义信息有利于分类，而低级语义信息则保留了较多的位置信息。 网络设计 网络为了规避deeplab的缺陷，放弃了空洞卷积，同时为了译码时生成较好的结果，利用高层语义信息来指导低层语义分辨率的恢复。网络总体设计如下： 网络放弃了空洞卷积，但为了获取更加丰富的语义信息，这里利用了一种金字塔结构来进行多尺度信息的获取，同时增加了全局平均池化作为辅助。 目前已知缓慢增加图片的分辨率恢复的效果较好，同时，文中设计了一种注意力机制，通过高级语义信息生成关于通道的注意力，依次来指导图片的恢复。 七、Adaptive Affinity Fields for Semantic Segmentation目的：语义分割能够实现像素级的分割，但对于一些语义信息不明显的区域区分不够好，如前景与背景太过相似，就难以实现较好的分割效果。论文为了解决这个问题，引入了对结构的推理，提出了自适应相似场。在实质上，其实是对损失函数的修改，将整个问题看作一个最优化问题。 相关信息： 目前也有一些利用结构推理来帮助分割的方法，例如CRF，可以从视觉上来匹配目标的相似度，可以被用作分割网络的后处理阶段，用来改善分割效果，但花费的推理时间较长，同时对变化比较敏感；GAN也可以用来改善分割效果，但不易训练，同时容易出现模式崩溃。 解决方案 目前大多数的分割网络的损失函数都是以像素点为对象的交叉熵损失函数，这样就只关注了像素，而缺失了对像素之间关系的考量，例如一辆车行驶在路上，我们很容易从两者的关系而确定目标是车辆。因此，文章在损失函数中引入了像素点的邻域。具体公式见论文。 具体来说，其使用的相似域损失函数是一种基于KL散度的相似域损失函数，详情参见论文。这种损失函数并非定义在图像上，而是定义在groundtruth上，通过groundtruth的引导，强迫同一类的聚合在一起，不同类的分离，而不管图像是什么样的。值得一提的是这个过程只发生在训练阶段，因此不耗费推理的时间。 对于CNN来说，一般都要求同样大小的卷积核，这其实是不合理的，论文提出了一种自适应的相似域损失函数，给与了不同类别不同的权重。具体公式见论文。 这样直接进行优化，会导致得到的最优解是一个非常小的值，不能达到自适应的目的，因此文中考虑了一种极端的方式，即在最坏的情况下找到最好的值，将其表述为一个极小极大问题，具体公式见论文。PS:这篇论文的公式太难打了，博客都显示不了，最后只能全部删除。]]></content>
      <categories>
        <category>论文</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[语义分割部分文章概览（一）]]></title>
    <url>%2F2018%2F12%2F19%2F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E9%83%A8%E5%88%86%E6%96%87%E7%AB%A0%E6%A6%82%E8%A7%88%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[此部分是关于部分语义分割论文的大致浏览 一、The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation论文主要是利用了DenseNet杰出的性能，将其应用到了语义分割的任务之中，并对网络的结构做了细微的调整，使其更容易训练，最终达到了较好的分割结果。如图所示：为了更好的进行上采样，仅在Densenet层之后进行跨层连接，减小训练的难度。 二、DenseASPP for Semantic Segmentation in Street Scenes论文认为，对于较大分辨率的图片，获取更大的感受野与提取多尺度信息是两个比较重要的问题。为了同时解决这两个问题，论文在ASPP（空洞金字塔池化）上做了改动，使用了一种密集连接的方式（个人认为参考了Densenet网络），实现了较好的分割效果。通过这样一种串行的方式，的确取得了远大于ASPP的感受野，同时实现了多尺度的提取。 三、BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation论文研究了如何在不牺牲精度跟分辨率的前提下保证较高的实时分割效率。网络结构如下所示：为了保证在语义分割中比较重要的空间分辨率与感受野，论文设计了两条路径，一条是空间路径，由三层卷积组成，目的是取得分辨率较高的1/8特征图；一条是语义路径，使用了一种轻量化模型Xception来快速增加感受野，同时做了一个最大池化操作使得感受野最大化。为了融合两部分的特征图，论文提出了一个特征融合模块与注意力优化模块。同时，为了辅助训练，在主损失函数之外还增加了两个辅助损失函数进行辅助训练。 四、ICNet for Real-Time Semantic Segmentation on High-Resolution Images问题的引入与介绍论文使用了多分辨率分支与标签引导来处理实时语义分割的问题，利用了PSPnet网络的基础框架，虽然略微牺牲了网络的分割精度，但大大缩短了网络推理的速度。论文主要有三个贡献： 提出了一种创新的图像级联网络，利用了低分辨率的语义信息与高分辨率的细节信息来进行语义分割。 提出的级联特征融合单元与级联标签导向来细化分割结果。 ICNet实现了5倍的加速，降低了5倍的内存。 网络设计 通过分析，计算复杂度与空间分辨率有较大关系，随着空间分辨率的提高，计算复杂度成平方的形式增加 直觉上来说，下采样输入、缩小特征图与模型压缩可以提升分割速度，实际上也确实如此，但这样会导致分割结果惨不忍睹。 所以作者设计了一种图像级联网络，语义分割的推理阶段最为耗费时间，所以这里使用了低分辨率的图片，使用的网络是经典分割网络；为了细化分割结果，又使用了较高分辨率的图片来补充信息，这里使用的是较为轻量化的网络。网络结构如下所示： 特征融合单元：融合单元如下图所示这里F1经过上采样之后还进行了空洞卷积，是为了融合近邻的语义。 级联标签引导：对groundtruth进行不同的下采样，以引导不同分支的训练，这样损失函数就有三项。在测试阶段，抛弃低分辨率与中分辨率的两支，只使用高分辨率的支路。 模型压缩：论文采用的压缩策略是一种渐进式的压缩，即先压缩部分，进行微调，再进行压缩，最终实现完全的压缩。]]></content>
      <categories>
        <category>论文</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[论文阅读-ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design]]></title>
    <url>%2F2018%2F12%2F08%2F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ShuffleNet-V2-Practical-Guidelines-for-Efficient-CNN-Architecture-Design%2F</url>
    <content type="text"><![CDATA[研究问题：进一步对模型进行精简，使其在移动端也具有良好的运行速度 亮点从目的出发找到设计原则，在设计原则的指导下来设计网络 相关信息 网络想要在移动端运行，则需要较低的运算复杂性，其衡量标准为浮点运算操作的数量，即FLOPs，目前大多数的网络都是通过组卷积与深度可分离卷积来降低FLOPs。 经过实验发现，降低了FLOPs，并不意味着网络的速度增加，FLOPs只是衡量速度的间接标准，我们需要找到衡量速度的直接标准。论文中经过实验发现，速度与内存访问成本（MAC）与网络的并行化程度有关。 实验与结论 卷积层的输入输出通道相等时，MAC最小。使用h与w代表输出特征图的尺寸，$c_1$ 与$c_2$代表输入与输出的通道数，则卷积层的FLOPs计算如下：$B = hw{c_1}{c_2}$，而对于MAC，对于1*1卷积，由于输入与输出特征图尺寸相同，所以有：${\rm MAC = }hw({c_1} + {c_2}) + {c_1}{c_2}$，由均值不等式，这里可以得到$hw({c_1} + {c_2}) + {c_1}{c_2} \ge 2hw\sqrt {c_1c_2} + {c_1}{c_2}$，将B代入可得：${\rm MAC}\ge{\rm2}\sqrt{hwB} + B/hw$，显然，等通道数相等时才能取得最小值。 过多的组卷积操作会增加MAC，使得速度变慢。对于1*1卷积来说，g代表组数，当使用了组卷积之后，B变为$B = hw{c_1}{c_2}/g$，MAC变为${\rm MAC=}hw({c_1} + {c_2}) + {c_1}{c_2}/g$，此时有：\begin{array}{c} MAC = hw({c_1} + {c_2}) + {c_1}{c_2}/g\\ = hw{c_1} + \frac{(Bg)}{(c_1)} + \frac{B}{(hw)} \end{array} 显然，当B不变时，增大g会增大MAC 模型中的分支数量越少，模型的速度越快 element-wise操作对速度影响非常大。 网络设计论文做了一个简单的操作，即通道分割，简单起见，假设分为两组，这里满足第三条原则；其中一组不做任何操作，满足第三条原则；一组进行深度可分离卷积，注意，这里保证输入输出通道不变，以满足第一条原则；组合的时候使用通道的组合，而不是相加，满足第四条原则。与shufflenetv1相比，下采样的操作多了一个1*1的卷积来混合特征。]]></content>
      <categories>
        <category>论文</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[论文阅读-CBAM: Convolutional Block Attention Module]]></title>
    <url>%2F2018%2F12%2F02%2F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-CBAM-Convolutional-Block-Attention-Module%2F</url>
    <content type="text"><![CDATA[研究问题：在给定特征图下，通过通道和空间两个维度推导出注意力图，然后将注意力图乘到输入的特征图上以用来自适应的细化输入的特征。设计的CBAM是一个轻量的模块，可以方便的集成在任意的CNN模型中。 相关信息：1、VGGNet通过堆积相同的形状的块并不会使得结果变差；ResNet使用了跳跃连接与相同的拓扑残差块构建了非常深的网络；GoogleNet表明增加网络的宽度可以改善网络的性能；Xception和ResNeXt表明基数比深度与宽度这两个因素具有更强的表示能力。 2、注意力机制不仅可以将注意力集中在感兴趣的区域，还能够提高感兴趣区域的表现力。论文的目的就在于利用注意力机制，关注重要的特征并抑制不重要的特征。 3、卷积操作本质是通过跨通道与空间来提取特征，所以论文就分别设计了通道与空间两个注意力模块。 网络架构：1、分为两个子模块，一个是通道注意力模块，一个是空间注意力模块。具体原理如下：给定一个特征图$F \in {\mathbb{R}^{C \times H \times W}}$，生成的通道注意力图为${M_c} \in {\mathbb{R}^{C \times 1 \times 1}}$，空间注意力图为${M_s} \in {\mathbb{R}^{1 \times H \times W}}$，对注意力过程总结如下： \begin{array}{c} {M_c}(F) = \sigma (MLP(AvgPool(F)) + MLP(MaxPool(F)))\\ = \sigma ({W_1}({W_0}(F_{avg}^c)) + {W_1}({W_0}(F_{\max }^c))) \end{array}概括来说，就是输入特征图依次与通道注意力图与空间注意力图作点乘，最终得到更加细化的特征图。 2、1）通道注意力模块： 通道注意力图主要是由通道之间的关系得到，过去人们常用平均池化来聚合空间信息，但论文认为最大池化或许能捕获到另外一些重要的特征，所以论文同时使用了平均池化与最大池化。为了降低运算量，首先对特征图的空间维度进行了压缩，之后并行的两种池化操作，将结果输入到一个多层感知机中，最后将结果对应元素相加即得到通道注意力图，公式表示如下： 2）空间注意力模块 通道注意力关注的是目标是什么，而空间注意力则关注的是目标在哪，二者可以相互补充。论文在通道轴上使用了最大池化与平均池化并将其结合起来，之后通过一个标准的卷积操作，得到空间注意力图。其用公式总结如下： 3、经过试验验证，模块插入方式如下：]]></content>
      <categories>
        <category>论文</category>
      </categories>
  </entry>
</search>
